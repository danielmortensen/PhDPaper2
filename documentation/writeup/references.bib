
@article{friedman_greedy_nodate,
	title = {{GREEDY} {FUNCTION} {APPROXIMATION}: A {GRADIENT} {BOOSTING} {MACHINE}},
	author = {Friedman, Jerome H},
}

@article{abel_exploratory_nodate,
	title = {Exploratory Gradient Boosting for Reinforcement Learning in Complex Domains},
	author = {Abel, David and Agarwal, Alekh and Diaz, Fernando and Krishnamurthy, Akshay and Schapire, Robert E},
}

@article{wiering_ensemble_2008,
	title = {Ensemble Algorithms in Reinforcement Learning},
	shortjournal = {{IEEE} Trans. Syst., Man, Cybern. B},
	author = {Wiering, M.A. and van Hasselt, H.},
	date = {2008-08},
}

@article{ernst_tree-based_nodate,
	title = {Tree-Based Batch Mode Reinforcement Learning},
	abstract = {Reinforcement learning aims to determine an optimal control policy from interaction with a system or from observations gathered from a system. In batch mode, it can be achieved by approximating the so-called Q-function based on a set of four-tuples (xt , ut , rt , xt+1) where xt denotes the system state at time t, ut the control action taken, rt the instantaneous reward obtained and xt+1 the successor state of the system, and by determining the control policy from this Q-function. The Q-function approximation may be obtained from the limit of a sequence of (batch mode) supervised learning problems. Within this framework we describe the use of several classical tree-based supervised learning methods ({CART}, Kd-tree, tree bagging) and two newly proposed ensemble algorithms, namely extremely and totally randomized trees. We study their performances on several examples and ﬁnd that the ensemble methods based on regression trees perform well in extracting relevant information about the optimal control policy from sets of four-tuples. In particular, the totally randomized trees give good results while ensuring the convergence of the sequence, whereas by relaxing the convergence constraint even better accuracy results are provided by the extremely randomized trees.},
	pages = {54},
	author = {Ernst, Damien and Geurts, Pierre and Wehenkel, Louis},
	langid = {english},
	file = {Ernst et al. - Tree-Based Batch Mode Reinforcement Learning.pdf:/home/daniel/Zotero/storage/QGUY72JJ/Ernst et al. - Tree-Based Batch Mode Reinforcement Learning.pdf:application/pdf},
}

@book{sutton_reinforcement_nodate,
	title = {Reinforcement Learning: An Introduction},
	author = {Sutton, Richard S and Barto, Andrew G},
	langid = {english},
	file = {Sutton and Barto - Reinforcement Learning An Introduction.pdf:/home/daniel/Zotero/storage/SI9AA8PQ/Sutton and Barto - Reinforcement Learning An Introduction.pdf:application/pdf},
}

@inproceedings{wiering_qv_2009,
	location = {Nashville, {TN}, {USA}},
	title = {The {QV} family compared to other reinforcement learning algorithms},
	isbn = {978-1-4244-2761-1},
	url = {http://ieeexplore.ieee.org/document/4927532/},
	doi = {10.1109/ADPRL.2009.4927532},
	abstract = {This paper describes several new online model-free reinforcement learning ({RL}) algorithms. We designed three new reinforcement algorithms, namely: {QV}2, {QVMAX}, and {QVMAX}2, that are all based on the {QV}-learning algorithm, but in contrary to {QV}-learning, {QVMAX} and {QVMAX}2 are off-policy {RL} algorithms and {QV}2 is a new on-policy {RL} algorithm. We experimentally compare these algorithms to a large number of different {RL} algorithms, namely: Q-learning, Sarsa, R-learning, Actor-Critic, {QV}-learning, and {ACLA}. We show experiments on ﬁve maze problems of varying complexity. Furthermore, we show experimental results on the cart pole balancing problem. The results show that for different problems, there can be large performance differences between the different algorithms, and that there is not a single {RL} algorithm that always performs best, although on average {QV}-learning scores highest.},
	eventtitle = {2009 {IEEE} Symposium on Adaptive Dynamic Programming and Reinforcement Learning ({ADPRL})},
	pages = {101--108},
	booktitle = {2009 {IEEE} Symposium on Adaptive Dynamic Programming and Reinforcement Learning},
	publisher = {{IEEE}},
	author = {Wiering, Marco A. and van Hasselt, Hado},
	urldate = {2020-12-11},
	date = {2009-03},
	langid = {english},
	file = {Wiering and van Hasselt - 2009 - The QV family compared to other reinforcement lear.pdf:/home/daniel/Zotero/storage/38Y5LLS2/Wiering and van Hasselt - 2009 - The QV family compared to other reinforcement lear.pdf:application/pdf},
}

@article{fujimoto_addressing_2018,
	title = {Addressing Function Approximation Error in Actor-Critic Methods},
	url = {http://arxiv.org/abs/1802.09477},
	abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of {OpenAI} gym tasks, outperforming the state of the art in every environment tested.},
	journaltitle = {{arXiv}:1802.09477 [cs, stat]},
	author = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
	urldate = {2020-12-11},
	date = {2018-10-22},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1802.09477},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-C.pdf:/home/daniel/Zotero/storage/UW8B4HI8/Fujimoto et al. - 2018 - Addressing Function Approximation Error in Actor-C.pdf:application/pdf},
}

@article{haarnoja_soft_2018,
	title = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
	url = {http://arxiv.org/abs/1801.01290},
	shorttitle = {Soft Actor-Critic},
	abstract = {Model-free deep reinforcement learning ({RL}) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep {RL} algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep {RL} methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
	journaltitle = {{arXiv}:1801.01290 [cs, stat]},
	author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
	urldate = {2020-12-11},
	date = {2018-08-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1801.01290},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep.pdf:/home/daniel/Zotero/storage/J54P7QYW/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep.pdf:application/pdf},
}

@article{lecue_robust_2017,
	title = {Robust machine learning by median-of-means : theory and practice},
	url = {http://arxiv.org/abs/1711.10306},
	shorttitle = {Robust machine learning by median-of-means},
	abstract = {We introduce new estimators for robust machine learning based on median-of-means ({MOM}) estimators of the mean of real valued random variables. These estimators achieve optimal rates of convergence under minimal assumptions on the dataset. The dataset may also have been corrupted by outliers on which no assumption is granted. We also analyze these new estimators with standard tools from robust statistics. In particular, we revisit the concept of breakdown point. We modify the original deﬁnition by studying the number of outliers that a dataset can contain without deteriorating the estimation properties of a given estimator. This new notion of breakdown number, that takes into account the statistical performances of the estimators, is non-asymptotic in nature and adapted for machine learning purposes. We proved that the breakdown number of our estimator is of the order of number of observations * rate of convergence. For instance, the breakdown number of our estimators for the problem of estimation of a d-dimensional vector with a noise variance σ2 is σ2d and it becomes σ2s log(ed/s) when this vector has only s non-zero component. Beyond this breakdown point, we proved that the rate of convergence achieved by our estimator is number of outliers divided by number of observations.},
	journaltitle = {{arXiv}:1711.10306 [math, stat]},
	author = {Lecué, Guillaume and Lerasle, Matthieu},
	urldate = {2020-12-11},
	date = {2017-11-30},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1711.10306},
	keywords = {Mathematics - Statistics Theory},
	file = {Lecué and Lerasle - 2017 - Robust machine learning by median-of-means  theor.pdf:/home/daniel/Zotero/storage/6TU926UF/Lecué and Lerasle - 2017 - Robust machine learning by median-of-means  theor.pdf:application/pdf},
}

@article{smith_optimizers_2006,
	title = {The Optimizer’s Curse: Skepticism and Postdecision Surprise in Decision Analysis},
	volume = {52},
	issn = {0025-1909, 1526-5501},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/mnsc.1050.0451},
	doi = {10.1287/mnsc.1050.0451},
	shorttitle = {The Optimizer’s Curse},
	pages = {311--322},
	number = {3},
	journaltitle = {Management Science},
	shortjournal = {Management Science},
	author = {Smith, James E. and Winkler, Robert L.},
	urldate = {2021-01-26},
	date = {2006-03},
	langid = {english},
	file = {Smith and Winkler - 2006 - The Optimizer’s Curse Skepticism and Postdecision.pdf:/home/daniel/Zotero/storage/EDXPIKPH/Smith and Winkler - 2006 - The Optimizer’s Curse Skepticism and Postdecision.pdf:application/pdf},
}

@inproceedings{badia_never_2020,
	title = {Never Give Up: Learning Directed Exploration Strategies},
	abstract = {We propose a reinforcement learning agent to solve hard exploration games by learning a range of directed exploratory policies. We construct an episodic memory-based intrinsic reward using k-nearest neighbors over the agent's recent experience to train the directed exploratory policies, thereby encouraging the agent to repeatedly revisit all states in its environment. A self-supervised inverse dynamics model is used to train the embeddings of the nearest neighbour lookup, biasing the novelty signal towards what the agent can control. We employ the framework of Universal Value Function Approximators to simultaneously learn many directed exploration policies with the same neural network, with different trade-offs between exploration and exploitation. By using the same neural network for different degrees of exploration/exploitation, transfer is demonstrated from predominantly exploratory policies yielding effective exploitative policies. The proposed method can be incorporated to run with modern distributed {RL} agents that collect large amounts of experience from many actors running in parallel on separate environment instances. Our method doubles the performance of the base agent in all hard exploration in the Atari-57 suite while maintaining a very high score across the remaining games, obtaining a median human normalised score of 1344.0\%. Notably, the proposed method is the first algorithm to achieve non-zero rewards (with a mean score of 8,400) in the game of Pitfall! without using demonstrations or hand-crafted features.},
	eventtitle = {{ICLR}},
	author = {Badia, Adrià and Sprechmann, Pablo},
	date = {2020},
	file = {never_give_up_learning_directed_exploration_strategies-Original Pdf.pdf:/home/daniel/Zotero/storage/KR7VHP47/never_give_up_learning_directed_exploration_strategies-Original Pdf.pdf:application/pdf},
}

@inproceedings{sun_program_2020,
	title = {Program Guided Agent},
	abstract = {Developing agents that can learn to follow natural language instructions has been an emerging research direction. While being accessible and flexible, natural language instructions can sometimes be ambiguous even to humans. To address this, we propose to utilize programs, structured in a formal language, as a precise and expressive way to specify tasks. We then devise a modular framework that learns to perform a task specified by a program – as different circumstances give rise to diverse ways to accomplish the task, our framework can perceive which circumstance it is currently under, and instruct a multitask policy accordingly to fulfill each subtask of the overall task. Experimental results on a 2D Minecraft environment not only demonstrate that the proposed framework learns to reliably accomplish program instructions and achieves zero-shot generalization to more complex instructions but also verify the efficiency of the proposed modulation mechanism for learning the multitask policy. We also conduct an analysis comparing various models which learn from programs and natural language instructions in an end-to-end fashion.},
	eventtitle = {{ICLR}},
	author = {Sun, Shao-Hua},
	date = {2020},
	file = {program_guided_agent-Original Pdf.pdf:/home/daniel/Zotero/storage/3QG4MG8E/program_guided_agent-Original Pdf.pdf:application/pdf},
}

@inproceedings{houbbadi_optimal_2019,
	title = {Optimal Charging Strategy to Minimize Electricity Cost and Prolong Battery Life of Electric Bus Fleet},
	booktitle = {Vehicle Power and Propulsion Conference},
	author = {A.~Houbbadi and E.~Redondo-Iglesias and R.~Trigui and S.~Pelissier and T.~Bouton},
	date = {2019-10},
}

@article{csonka_optimization_2021,
	title = {Optimization of Static and Dynamic Charging Infrastructure for Electric Buses},
	journaltitle = {Energies},
	author = {B.~Csonka},
	date = {2021-06-13},
}

@article{zhou_bi-objective_2021,
	title = {Bi-Objective Optimization for Battery Electric Bus Deployment Considering Cost and Environmental Equity},
	shortjournal = {Trans. Intell. Transport. Syst.},
	author = {Y.~Zhou and X.C.~Liu and R.~Wei and A.~Golub},
	date = {2021-04},
}

@article{qin_numerical_2016,
	title = {Numerical analysis of electric bus fast charging strategies for demand charge reduction},
	shortjournal = {Transportation Research Part A: Policy and Practice},
	author = {N.~Qin and A.~Gusrialdi and P.R.~Brooker and A.~T-Raissi},
	date = {2016-12},
}

@article{wei_optimizing_2018,
	title = {Optimizing the spatio-temporal deployment of battery electric bus system},
	shortjournal = {Journal of Transport Geography},
	author = {Wei, Ran and Liu, Xiaoyue and Ou, Yi and Kiavash Fayyaz, S.},
	date = {2018-04},
}

@article{bagherinezhad_spatio-temporal_2020,
	title = {Spatio-Temporal Electric Bus Charging Optimization With Transit Network Constraints},
	volume = {56},
	pages = {5741--5749},
	number = {5},
	journaltitle = {{IEEE} Trans. on Ind. Applicat.},
	author = {A.~Bagherinezhad and A.~D.~Palomino and B.~Li and M.~Parvania},
	date = {2020-09},
}

@inproceedings{boonraksa_impact_2019,
	title = {Impact of Electric Bus Charging on the Power Distribution System a Case Study {IEEE} 33 Bus Test System},
	booktitle = {2019 {IEEE} {PES} {GTD} Grand Internat. Conf. and Exposition Asia},
	author = {T.~Boonraksa and A.~Paudel and P.~Dawan and B.~Marungsri},
	date = {2019-03},
}

@inproceedings{poornesh_comparative_2020,
	title = {A Comparative study on Electric Vehicle and Internal Combustion Engine Vehicles},
	booktitle = {2020 International Conference on Smart Electronics and Communication},
	author = {K.~Poornesh and K.P.~Nivya and K.~Sireesha},
	date = {2020-09},
}

@inproceedings{li_research_2015,
	location = {Changsha, China},
	title = {Research into probabilistic representation of electric vehicle's charging load and its effect to the load characteristics of the network},
	isbn = {978-1-4673-7106-3},
	url = {http://ieeexplore.ieee.org/document/7432270/},
	doi = {10.1109/DRPT.2015.7432270},
	eventtitle = {2015 5th International Conference on Electric Utility Deregulation and Restructuring and Power Technologies ({DRPT})},
	pages = {426--430},
	booktitle = {2015 5th International Conference on Electric Utility Deregulation and Restructuring and Power Technologies ({DRPT})},
	publisher = {{IEEE}},
	author = {Li, Yanning and Zhang, Jinguo},
	urldate = {2021-11-13},
	date = {2015-11},
	file = {Li and Zhang - 2015 - Research into probabilistic representation of elec.pdf:/home/daniel/Zotero/storage/89HW67GX/Li and Zhang - 2015 - Research into probabilistic representation of elec.pdf:application/pdf},
}

@inproceedings{poornesh_comparative_2020-1,
	location = {Trichy, India},
	title = {A Comparative study on Electric Vehicle and Internal Combustion Engine Vehicles},
	isbn = {978-1-72815-461-9},
	url = {https://ieeexplore.ieee.org/document/9215386/},
	doi = {10.1109/ICOSEC49089.2020.9215386},
	abstract = {Electrification of the vehicles gains a significant research importance due to the increasing amount of greenhouse gas ({GHG}) emission by using conventional internal combustion engine vehicles ({ICEV}). The major benefits of electric vehicles ({EVs}) are a reduced amount of carbon monoxide ({CO}) emissions, higher efficiency, performance, and lower maintenance costs. It also allows energy diversification by switching to renewable resources. This paper analyzes the efficiency of an {EV} with {ICEV} by considering various parameters like required torque, speed and distance traveled. The simulation is developed in {MATLAB}/Simulink and analysis is presented for various performance metrics as a measure.},
	eventtitle = {2020 International Conference on Smart Electronics and Communication ({ICOSEC})},
	pages = {1179--1183},
	booktitle = {2020 International Conference on Smart Electronics and Communication ({ICOSEC})},
	publisher = {{IEEE}},
	author = {Poornesh, Kavuri and Nivya, Kuzhivila Pannickottu and Sireesha, K.},
	urldate = {2021-11-13},
	date = {2020-09},
	langid = {english},
	file = {Poornesh et al. - 2020 - A Comparative study on Electric Vehicle and Intern.pdf:/home/daniel/Zotero/storage/MNUADQMB/Poornesh et al. - 2020 - A Comparative study on Electric Vehicle and Intern.pdf:application/pdf},
}

@inproceedings{zhou_optimization_2018,
	title = {Optimization Method of Fast Charging Buses Charging Strategy for Complex Operating Environment},
	booktitle = {2nd {IEEE} Conf. on Energy Internet and Energy System Integration ({EI}2)},
	author = {D.~Zhou and Z.~Ren and K.~Sun and H.~Dai},
	date = {2018-10},
}

@inproceedings{stahleder_impact_2019,
	title = {Impact Assessment of High Power Electric Bus Charging on Urban Distribution Grids},
	booktitle = {{IECON} 2019 - 45th Annual Conference of the {IEEE} Industrial Electronics Society},
	author = {D.Stahleder and D.~Reihs and S.~Ledinger and F.~Lehfuss},
	date = {2019-10},
}

@inproceedings{deb_impact_2017,
	title = {Impact of electric vehicle charging stations on reliability of distribution network},
	booktitle = {2017 Internat. Conf. on Tech. Adv. in Power and Energy ( {TAP} Energy)},
	author = {S.~Deb and K.~Kalita and P.~Mahanta},
	date = {2017-12},
}

@inproceedings{ojer_development_2020,
	title = {Development of energy management strategies for the sizing of a fast charging station for electric buses},
	booktitle = {{IEEE} International Conference on Environment and Electrical Engineering},
	author = {I.~Ojer and A.~Berrueta and J.~Pascual and P.~Sanchis and A.~Ursua},
	date = {2020-06},
}

@article{zhuang_stochastic_2021,
	title = {Stochastic Energy Management of Electric Bus Charging Stations With Renewable Energy Integration and B2G Capabilities},
	volume = {12},
	issn = {1949-3029, 1949-3037},
	url = {https://ieeexplore.ieee.org/document/9266099/},
	doi = {10.1109/TSTE.2020.3039758},
	abstract = {In this paper, the stochastic energy management of electric bus charging stations ({EBCSs}) is investigated, where the photovoltaic ({PV}) with integrated battery energy storage systems ({BESS}) and bus-to-grid (B2G) capabilities of electric buses ({EBs}) are included for cost-effective charging of {EBs}. Also, the day-ahead dynamic prices are derived to mitigate charging impacts on power distribution systems. This problem is formulated as a distributionally robust Markov decision process ({DRMDP}) with uncertain transition probabilities and costs to address the impacts of random bus loads with inaccurate probability density function estimation. An event-based ambiguity set with combined statistical distance and moment information is developed to achieve minimax-regret criteria for less-conservative and robust solutions. To facilitate practical applications with reduced computational complexity, a heuristic regret function is proposed, based on which the dynamic prices are derived. Case studies based on {EB} data from St. Albert Transit and {IEEE} test feeders indicate that the proposed method can minimize {EB} charging cost with mitigated impacts on power distribution systems.},
	pages = {1206--1216},
	number = {2},
	journaltitle = {{IEEE} Transactions on Sustainable Energy},
	shortjournal = {{IEEE} Trans. Sustain. Energy},
	author = {Zhuang, Peng and Liang, Hao},
	urldate = {2021-11-16},
	date = {2021-04},
	langid = {english},
	file = {Zhuang and Liang - 2021 - Stochastic Energy Management of Electric Bus Charg.pdf:/home/daniel/Zotero/storage/35ZRYTZG/Zhuang and Liang - 2021 - Stochastic Energy Management of Electric Bus Charg.pdf:application/pdf},
}

@inproceedings{jahic_preemptive_2019,
	title = {Preemptive vs. non-preemptive charging schedule for large-scale electric bus depots},
	booktitle = {2019 {IEEE} {PES} Innovative Smart Grid Technologies Europe},
	author = {A.~Jahic and M.~Eskander and D.~Schulz},
	date = {2019-09},
}

@inproceedings{xian_zhang_optimal_2016,
	title = {Optimal dispatch of electric vehicle batteries between battery swapping stations and charging stations},
	booktitle = {2016 {IEEE} Power and Energy Society General Meeting},
	author = {Z.~Xian and G.~Wang},
	date = {2016-07},
}

@inproceedings{jain_battery_2020,
	title = {Battery Swapping Technology},
	booktitle = {2020 5th {IEEE} International Conference on Recent Advances and Innovations in Engineering},
	author = {Jain, Shubham and Ahmad, Zaurez and Alam, Mohammad Saad and Rafat, Yasser},
	date = {2020-12-01},
}

@inproceedings{balde_electric_2019,
	title = {Electric Road system With Dynamic Wireless charging of Electric buses},
	booktitle = {2019 {IEEE} Trans. Electrification Conf.},
	author = {B.~J.~Balde and A.~Sardar},
	date = {2019-12},
}

@inproceedings{jeong_automatic_2018,
	title = {Automatic Current Control by Self-Inductance Variation for Dynamic Wireless {EV} Charging},
	booktitle = {{IEEE} {PELS} Workshop on Emerging Tech.: Wireless Power Transfer},
	author = {S.~Y.~Jeong and J.~H.~Park and G.~P.~Hong and C.~T.~Rim},
	date = {2018-06},
}

@inproceedings{el-taweel_incorporation_2019,
	title = {Incorporation of Battery Electric Buses in the Operation of Intercity Bus Services},
	booktitle = {2019 {IEEE} Transportation Electrification Conference and Expo ({ITEC})},
	author = {N.A.~El-Taweel and E.Z.~Farag},
	date = {2019-06},
}

@article{whitaker_network_2021,
	title = {A Network Flow Approach to Battery Electric Bus Scheduling},
	author = {J.~Whitaker and G.~Droge and M.~Hansen and D.~Mortensen and J.~Gunther},
	date = {2021},
}

@inproceedings{gao_charging_2019,
	title = {Charging Load Forecasting of Electric Vehicle Based on Monte Carlo and Deep Learning},
	booktitle = {2019 {IEEE} Sus. Power and Energy Conf.},
	author = {Q.~Gao and Z.~Lin and T.~Zhu and W.~Zhou and G.~Wang and T.~Zhang and Z.~Zhang and M.~Waseem and S.~Liu and C.~Han},
	date = {2019-11},
}

@inproceedings{clairand_impact_2020,
	location = {Montreal, {QC}, Canada},
	title = {The impact of charging electric buses on the power grid},
	isbn = {978-1-72815-508-1},
	url = {https://ieeexplore.ieee.org/document/9282014/},
	doi = {10.1109/PESGM41954.2020.9282014},
	abstract = {Public transportation, such as buses, depends mostly on fossil fuels, which results in signiﬁcant environmental and health issues, especially for cities with high altitudes. The integration of electric buses ({EBs}) has been carried out as one of the alternatives to electrify public transportation. Although {EBs} will help to overcome these issues, a massive penetration of {EBs} will also lead to several technical issues. Moreover, their challenges compared to typical electric vehicles ({EVs}) are more complex due to the lack of ﬂexibility and temporal correlation of scheduling for public transportation. This paper studies the effect of charging {EBs} in a bus station in the power load, and proposes a new methodology considering {EB} aggregators to minimize electricity costs while meeting grid constraints. Moreover, a sensitivity analysis is performed to evaluate the impact of the number of {EB} chargers in the charging costs.},
	eventtitle = {2020 {IEEE} Power \& Energy Society General Meeting ({PESGM})},
	pages = {1--5},
	booktitle = {2020 {IEEE} Power \& Energy Society General Meeting ({PESGM})},
	publisher = {{IEEE}},
	author = {Clairand, Jean-Michel and Gonzalez-Roriguez, Mario and Teran, Paulo Guerra and Cedenno, Irvin and Escriva-Escriva, Guillermo},
	urldate = {2021-11-19},
	date = {2020-08-02},
	langid = {english},
	file = {Clairand et al. - 2020 - The impact of charging electric buses on the power.pdf:/home/daniel/Zotero/storage/ZJERFUT6/Clairand et al. - 2020 - The impact of charging electric buses on the power.pdf:application/pdf},
}

@article{cheng_smart_2020,
	title = {A smart charging algorithm-based fast charging station with energy storage system-free},
	journal = {CSEE Journal of Power and Energy Systems},
	author = {Q.~Cheng and L.~Chen and R.~Wang and D.~Ma and D.~Qin},
	date = {2020},
}

@book{moon_mathematical_2000,
	location = {United State of America},
	edition = {2},
	title = {Mathematical Methods and Algorithms for Signal Processing},
	isbn = {0-201-36186-8},
	pagetotal = {937},
	publisher = {Marsha Horton},
	author = {Moon, Todd and Stirling, Wynn},
	date = {2000},
}

@legislation{noauthor_rocky_nodate,
	author={Rocky Mountain Power},
	title = {Rocky Mountain Power Electric Service Schedule No. 8 State of Utah},
	urldate = {2022-02-03},
}
